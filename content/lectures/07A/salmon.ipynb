{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Salmon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a generative model that simulates fragments (and their corresponding reads) from a known transcriptome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\l'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\l'\n",
      "/tmp/ipykernel_19556/4027555006.py:11: SyntaxWarning: invalid escape sequence '\\l'\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def simulate_rna_seq_reads(\n",
    "    transcripts: dict[str, str],\n",
    "    alpha: list[int],\n",
    "    num_reads: int = 10_000,\n",
    "    frag_len_dist=None,\n",
    "    seq_bias_5prime=None,\n",
    "    seq_bias_3prime=None,\n",
    "    gc_bias=None,\n",
    "    rng=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulate reads according to a generative model similar to Salmon.\n",
    "\n",
    "    $$\n",
    "    T = {t_1, t_2, \\ldots, t_M}.\n",
    "    $$\n",
    "\n",
    "    Args:\n",
    "        transcripts: A dictionary `{ transcript_id : transcript_sequence }`.\n",
    "        alpha: Transcript abundance proportions (must sum to 1).\n",
    "        num_reads : int\n",
    "            Total number of reads (fragments) to simulate.\n",
    "        frag_len_dist : function -> int\n",
    "            A function that samples from the chosen fragment length distribution.\n",
    "            E.g. a lambda that calls np.random.  If None, we’ll assume a simple fixed length.\n",
    "        seq_bias_5prime : function(str) -> float, optional\n",
    "            A function that, given the 5'-end context, returns a multiplicative bias.\n",
    "            If None, no 5'-end bias is applied.\n",
    "        seq_bias_3prime : function(str) -> float, optional\n",
    "            A function that, given the 3'-end context, returns a multiplicative bias.\n",
    "            If None, no 3'-end bias is applied.\n",
    "        gc_bias : function(float) -> float, optional\n",
    "            A function that, given GC content of a fragment, returns a multiplicative bias.\n",
    "        rng : np.random.Generator, optional\n",
    "            Random number generator for reproducibility. If None, a new default is created.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reads : list\n",
    "        A list of (transcript_id, start_pos, fragment_seq) for each simulated fragment.\n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # Gather transcript IDs and sequences\n",
    "    tx_ids: list[str] = list(transcripts.keys())\n",
    "    tx_seqs = list(transcripts.values())\n",
    "    M = len(tx_ids)\n",
    "\n",
    "    # Basic checks\n",
    "    alpha = np.array(alpha, dtype=float)\n",
    "    alpha /= alpha.sum()  # Ensure it sums to 1\n",
    "\n",
    "    # Default fragment length distribution if none is given\n",
    "    if frag_len_dist is None:\n",
    "        # Suppose we always sample fragments of length ~ 200 plus some noise\n",
    "        def frag_len_dist():\n",
    "            return max(25, int(rng.normal(200, 30)))  # just an example\n",
    "\n",
    "    reads = []\n",
    "\n",
    "    # Precompute transcript lengths\n",
    "    tx_lengths = np.array([len(seq) for seq in tx_seqs])\n",
    "\n",
    "    # For each read to simulate:\n",
    "    for _ in range(num_reads):\n",
    "        # 1. Choose a transcript index i ~ Multinomial(alpha)\n",
    "        i = rng.choice(M, p=alpha)\n",
    "\n",
    "        # 2. Sample fragment length\n",
    "        L = frag_len_dist()\n",
    "\n",
    "        # 3. Select a start position\n",
    "        #    We'll ensure there's room for a fragment of length L\n",
    "        max_start = max(0, tx_lengths[i] - L)\n",
    "        if max_start == 0:\n",
    "            # If the transcript is too short, skip or try smaller L\n",
    "            # In a real simulator, you'd handle corner cases more gracefully.\n",
    "            continue\n",
    "        start = rng.integers(low=0, high=max_start + 1)\n",
    "\n",
    "        # 4. Extract the fragment sequence from the transcript\n",
    "        frag_seq = tx_seqs[i][start : start + L]\n",
    "\n",
    "        # 5. Compute (multiplicative) bias from 5' sequence context\n",
    "        bias_factor = 1.0\n",
    "        kmer_len = 5  # e.g. consider 5bp context\n",
    "        if seq_bias_5prime:\n",
    "            # pad if near start\n",
    "            context_5prime = tx_seqs[i][max(0, start - kmer_len) : start]\n",
    "            bias_factor *= seq_bias_5prime(context_5prime)\n",
    "\n",
    "        # 6. Compute (multiplicative) bias from 3' sequence context\n",
    "        if seq_bias_3prime:\n",
    "            # pad if near end\n",
    "            context_3prime = tx_seqs[i][(start + L) : (start + L + kmer_len)]\n",
    "            bias_factor *= seq_bias_3prime(context_3prime)\n",
    "\n",
    "        # 7. Compute GC bias (entire fragment)\n",
    "        if gc_bias:\n",
    "            gc_frac = (frag_seq.count('G') + frag_seq.count('C')) / float(len(frag_seq))\n",
    "            bias_factor *= gc_bias(gc_frac)\n",
    "\n",
    "        # 8. Accept or reject fragment according to bias_factor.\n",
    "        #    A typical generative approach would weight transcripts by bias_factor,\n",
    "        #    but here we’ll do a simple “probabilistic acceptance” approach:\n",
    "        #    e.g., if bias_factor < 0.5, we might reject half the time, etc.\n",
    "        #    Or we can simply store the factor to re-weigh probabilities later.\n",
    "        if rng.random() < bias_factor:\n",
    "            # Store the read\n",
    "            reads.append((tx_ids[i], start, frag_seq))\n",
    "\n",
    "    return reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "Simulated 0 reads.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Suppose we have a tiny transcriptome of 2 transcripts\n",
    "transcripts_demo = {\n",
    "    \"txA\": \"AGCGTACGTACTGACGTAGCTAGCTAGCGTACCGTTAGCA\"*3,\n",
    "    \"txB\": \"CGTACGGGTTTACGATCGATCGTAGCGTACGATCATTAACC\"*2,\n",
    "}\n",
    "\n",
    "# 50:50 initial transcript proportions\n",
    "alpha_demo = [0.5, 0.5]\n",
    "\n",
    "# Example biases (very rough placeholders)\n",
    "def seq_bias_5prime_example(seq5):\n",
    "    # if seq5 ends with \"G\" often, let's prefer it\n",
    "    return 1.2 if seq5.endswith(\"G\") else 1.0\n",
    "\n",
    "def seq_bias_3prime_example(seq3):\n",
    "    return 1.1 if seq3.startswith(\"A\") else 1.0\n",
    "\n",
    "def gc_bias_example(gc_frac):\n",
    "    # Suppose we strongly favor mid-range GC\n",
    "    return np.exp(-5 * (gc_frac - 0.5)**2)\n",
    "\n",
    "simulated_reads = simulate_rna_seq_reads(\n",
    "    transcripts_demo,\n",
    "    alpha_demo,\n",
    "    num_reads=1000,\n",
    "    seq_bias_5prime=seq_bias_5prime_example,\n",
    "    seq_bias_3prime=seq_bias_3prime_example,\n",
    "    gc_bias=gc_bias_example\n",
    ")\n",
    "print(simulated_reads)\n",
    "print(\"Simulated {} reads.\".format(len(simulated_reads)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to perform abundance estimation (quantification).\n",
    "We can define an 'assignment matrix' X of shape (N, M), where:\n",
    "\n",
    "   X[i,k] = Probability that read i originated from transcript k.\n",
    "\n",
    "Given:\n",
    " - A set of N reads R = {r1, r2, ..., rN}\n",
    " - M transcripts T = {t1, t2, ..., tM}\n",
    " - Some auxiliary model p(r | t), e.g. a computed alignment likelihood \n",
    "   or a simpler approximate match function that reflects bias.\n",
    "\n",
    "We also have abundance parameters alpha_k >= 0, with sum(alpha_k) = 1.\n",
    "\n",
    "A typical maximum-likelihood approach (like in Salmon or an EM algorithm) \n",
    "tries to maximize p(R | alpha, bias-params). That is often solved via \n",
    "an EM loop, or by direct optimization. Below we give a minimal demonstration \n",
    "using SciPy’s \"minimize\" or \"least_squares\" to optimize alpha and X subject to constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def assignment_likelihood(read_likelihoods, alpha):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood of the reads given alpha, \n",
    "    ignoring normalizing constants. \n",
    "\n",
    "    read_likelihoods is shape (N, M),\n",
    "      read_likelihoods[i,k] = p(r_i | transcript k, bias-params).\n",
    "\n",
    "    alpha is shape (M,) with sum(alpha)=1, alpha >= 0.\n",
    "\n",
    "    The probability that read i came from transcript k is:\n",
    "      X[i,k] = alpha[k] * read_likelihoods[i,k]\n",
    "               / sum_j [alpha[j] * read_likelihoods[i,j]]\n",
    "\n",
    "    The negative log-likelihood is \n",
    "      -sum_i log( sum_k [ alpha[k] * read_likelihoods[i,k] ] ).\n",
    "    \"\"\"\n",
    "    # add small eps to avoid log(0)\n",
    "    eps = 1e-15\n",
    "    p_reads = np.sum(alpha * read_likelihoods, axis=1) + eps\n",
    "    return -np.sum(np.log(p_reads))\n",
    "\n",
    "def constraint_sum_to_one(alpha):\n",
    "    # sum(alpha) - 1 = 0 as a constraint\n",
    "    return np.sum(alpha) - 1.0\n",
    "\n",
    "def abundance_estimation(read_likelihoods, init_alpha=None):\n",
    "    \"\"\"\n",
    "    Use SciPy's minimize to optimize the negative log-likelihood\n",
    "    subject to sum(alpha)=1, alpha>=0.\n",
    "    \"\"\"\n",
    "    N, M = read_likelihoods.shape\n",
    "    if init_alpha is None:\n",
    "        init_alpha = np.ones(M) / M  # uniform init\n",
    "\n",
    "    # We'll use SLSQP to handle constraints\n",
    "    cons = ({'type': 'eq',  'fun': constraint_sum_to_one})\n",
    "    bnds = [(0.0, 1.0) for _ in range(M)]\n",
    "\n",
    "    # Objective for minimize\n",
    "    def objective(alpha):\n",
    "        return assignment_likelihood(read_likelihoods, alpha)\n",
    "\n",
    "    res = minimize(\n",
    "        objective,\n",
    "        init_alpha,\n",
    "        method='SLSQP',\n",
    "        bounds=bnds,\n",
    "        constraints=cons,\n",
    "        options={\"maxiter\": 100, \"ftol\": 1e-7},\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have 3 transcripts and 10 reads\n",
    "# read_lik[i,k] = p(r_i | t_k), which you might get from alignment or\n",
    "# a bias model. Here we just make up some numbers.\n",
    "read_lik = np.array([\n",
    "    [0.1,  0.01, 0.05],\n",
    "    [0.2,  0.10, 0.02],\n",
    "    [0.05, 0.06, 0.02],\n",
    "    [0.10, 0.02, 0.09],\n",
    "    [0.15, 0.04, 0.04],\n",
    "    [0.12, 0.07, 0.01],\n",
    "    [0.05, 0.02, 0.01],\n",
    "    [0.03, 0.07, 0.08],\n",
    "    [0.06, 0.03, 0.15],\n",
    "    [0.01, 0.10, 0.05],\n",
    "])\n",
    "\n",
    "# Run the optimization\n",
    "result = abundance_estimation(read_lik)\n",
    "\n",
    "if result.success:\n",
    "    print(\"Optimized alpha:\", result.x)\n",
    "    # We can reconstruct assignment matrix if needed:\n",
    "    # X[i, k] = alpha[k] * read_likelihoods[i,k] / sum_j[ alpha[j]*read_likelihoods[i,j] ]\n",
    "    alpha_opt = result.x\n",
    "    X = alpha_opt * read_lik\n",
    "    denom = X.sum(axis=1, keepdims=True)\n",
    "    X /= denom\n",
    "    print(\"Assignment matrix (first few reads):\\n\", X[:5])\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Bias Modeling**  \n",
    "   In the example generative function simulate_rna_seq_reads, we showed how you might incorporate 5′ context, 3′ context, and GC content biases multiplicatively when deciding whether or not to “accept” or “reject” a fragment for output. Salmon uses a more nuanced approach by weighting transcript likelihoods by the learned bias parameters and then performing inference over all reads collectively citeturn0file0.  \n",
    "\n",
    "2. **EM vs. Direct Constrained Optimization**  \n",
    "   In Salmon, the assignment matrix X (the conditional probability that read i comes from transcript k) is not stored explicitly; instead, it is computed in each E-step. However, the simple demonstration above uses scipy.optimize.minimize to show how you could do direct likelihood-based estimation subject to constraints (sum(alpha)=1, alpha ≥ 0). An alternative is to implement the EM loop manually, especially if you want to handle large data sets more efficiently (by collapsing equivalent reads, as Salmon does citeturn0file0).\n",
    "\n",
    "3. **Scalability**  \n",
    "   The above code snippets are geared toward clarity rather than speed or memory efficiency. Real RNA‑seq data sets can contain tens of millions of reads, so you would likely want to:\n",
    "   - **Collapse or bin reads** that share the same mapping profile (equivalence classes).\n",
    "   - **Use iterative or streaming algorithms** (like SCVB0 or online EM) rather than a large, all-in-memory assignment matrix.\n",
    "\n",
    "4. **Connection to Salmon**  \n",
    "   Salmon’s real implementation is more elaborate: it uses quasi-mapping or alignments, runs an online phase to estimate abundance and bias parameters, and then refines abundance in an offline phase using collapsed EM or VBEM citeturn0file0. The code here is merely a self-contained illustration of the underlying statistical ideas in Python.\n",
    "\n",
    "Using these building blocks, you can tailor the model details, incorporate advanced bias terms, or switch from SLSQP-based minimization to a hand-rolled EM iteration, depending on what best mirrors the behavior of Salmon or suits your needs for quantification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
